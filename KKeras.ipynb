{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.linspace(1,10,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=3*X+5+np.random.randn(200,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x225c0cdbef0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model,load_model\n",
    "from keras.layers import Dense,Dropout,Flatten,Input,Conv2D,MaxPool2D\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(1,))\n",
    "x=Dense(units=1)(inp)\n",
    "model=Model(inp,x)\n",
    "model.compile(optimizer='sgd',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144 samples, validate on 16 samples\n",
      "Epoch 1/200\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 49.9515 - val_loss: 4.5103\n",
      "Epoch 2/200\n",
      "144/144 [==============================] - 0s 104us/step - loss: 4.3989 - val_loss: 3.4568\n",
      "Epoch 3/200\n",
      "144/144 [==============================] - 0s 142us/step - loss: 4.0737 - val_loss: 3.3960\n",
      "Epoch 4/200\n",
      "144/144 [==============================] - 0s 120us/step - loss: 4.0980 - val_loss: 3.2030\n",
      "Epoch 5/200\n",
      "144/144 [==============================] - 0s 83us/step - loss: 3.8016 - val_loss: 3.3182\n",
      "Epoch 6/200\n",
      "144/144 [==============================] - 0s 72us/step - loss: 3.6090 - val_loss: 3.9436\n",
      "Epoch 7/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 3.5260 - val_loss: 3.5581\n",
      "Epoch 8/200\n",
      "144/144 [==============================] - 0s 132us/step - loss: 3.2827 - val_loss: 3.4238\n",
      "Epoch 9/200\n",
      "144/144 [==============================] - 0s 162us/step - loss: 3.1745 - val_loss: 2.6189\n",
      "Epoch 10/200\n",
      "144/144 [==============================] - 0s 104us/step - loss: 3.0512 - val_loss: 2.5536\n",
      "Epoch 11/200\n",
      "144/144 [==============================] - 0s 113us/step - loss: 2.9848 - val_loss: 2.7167\n",
      "Epoch 12/200\n",
      "144/144 [==============================] - 0s 93us/step - loss: 2.7681 - val_loss: 3.2756\n",
      "Epoch 13/200\n",
      "144/144 [==============================] - 0s 144us/step - loss: 2.6888 - val_loss: 3.0101\n",
      "Epoch 14/200\n",
      "144/144 [==============================] - 0s 147us/step - loss: 2.6448 - val_loss: 3.0618\n",
      "Epoch 15/200\n",
      "144/144 [==============================] - 0s 133us/step - loss: 2.6063 - val_loss: 3.3485\n",
      "Epoch 16/200\n",
      "144/144 [==============================] - 0s 161us/step - loss: 2.4546 - val_loss: 2.2829\n",
      "Epoch 17/200\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.500 - 0s 137us/step - loss: 2.2240 - val_loss: 2.6787\n",
      "Epoch 18/200\n",
      "144/144 [==============================] - 0s 112us/step - loss: 2.1898 - val_loss: 2.5799\n",
      "Epoch 19/200\n",
      "144/144 [==============================] - 0s 111us/step - loss: 2.1445 - val_loss: 1.9514\n",
      "Epoch 20/200\n",
      "144/144 [==============================] - 0s 119us/step - loss: 1.9926 - val_loss: 1.8050\n",
      "Epoch 21/200\n",
      "144/144 [==============================] - 0s 99us/step - loss: 1.9037 - val_loss: 1.7731\n",
      "Epoch 22/200\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.8567 - val_loss: 1.9967\n",
      "Epoch 23/200\n",
      "144/144 [==============================] - 0s 105us/step - loss: 1.7767 - val_loss: 1.7404\n",
      "Epoch 24/200\n",
      "144/144 [==============================] - 0s 143us/step - loss: 1.7215 - val_loss: 2.1503\n",
      "Epoch 25/200\n",
      "144/144 [==============================] - 0s 142us/step - loss: 1.6807 - val_loss: 1.8613\n",
      "Epoch 26/200\n",
      "144/144 [==============================] - 0s 162us/step - loss: 1.7239 - val_loss: 1.5468\n",
      "Epoch 27/200\n",
      "144/144 [==============================] - 0s 103us/step - loss: 1.6833 - val_loss: 1.6709\n",
      "Epoch 28/200\n",
      "144/144 [==============================] - 0s 146us/step - loss: 1.6011 - val_loss: 1.6264\n",
      "Epoch 29/200\n",
      "144/144 [==============================] - 0s 131us/step - loss: 1.5333 - val_loss: 1.6794\n",
      "Epoch 30/200\n",
      "144/144 [==============================] - 0s 153us/step - loss: 1.4550 - val_loss: 1.7444\n",
      "Epoch 31/200\n",
      "144/144 [==============================] - 0s 190us/step - loss: 1.5531 - val_loss: 1.5752\n",
      "Epoch 32/200\n",
      "144/144 [==============================] - 0s 139us/step - loss: 1.3808 - val_loss: 2.0958\n",
      "Epoch 33/200\n",
      "144/144 [==============================] - 0s 125us/step - loss: 1.4418 - val_loss: 1.3462\n",
      "Epoch 34/200\n",
      "144/144 [==============================] - 0s 129us/step - loss: 1.3459 - val_loss: 1.7128\n",
      "Epoch 35/200\n",
      "144/144 [==============================] - 0s 153us/step - loss: 1.3192 - val_loss: 1.3038\n",
      "Epoch 36/200\n",
      "144/144 [==============================] - 0s 140us/step - loss: 1.2819 - val_loss: 1.2813\n",
      "Epoch 37/200\n",
      "144/144 [==============================] - 0s 182us/step - loss: 1.2435 - val_loss: 1.4779\n",
      "Epoch 38/200\n",
      "144/144 [==============================] - 0s 125us/step - loss: 1.2575 - val_loss: 1.4155\n",
      "Epoch 39/200\n",
      "144/144 [==============================] - 0s 70us/step - loss: 1.1909 - val_loss: 1.2283\n",
      "Epoch 40/200\n",
      "144/144 [==============================] - 0s 88us/step - loss: 1.1593 - val_loss: 1.3239\n",
      "Epoch 41/200\n",
      "144/144 [==============================] - 0s 131us/step - loss: 1.1539 - val_loss: 1.8482\n",
      "Epoch 42/200\n",
      "144/144 [==============================] - 0s 83us/step - loss: 1.2051 - val_loss: 1.3200\n",
      "Epoch 43/200\n",
      "144/144 [==============================] - 0s 91us/step - loss: 1.0889 - val_loss: 1.1583\n",
      "Epoch 44/200\n",
      "144/144 [==============================] - 0s 147us/step - loss: 1.0810 - val_loss: 1.1455\n",
      "Epoch 45/200\n",
      "144/144 [==============================] - 0s 103us/step - loss: 1.1011 - val_loss: 1.1314\n",
      "Epoch 46/200\n",
      "144/144 [==============================] - 0s 78us/step - loss: 1.0883 - val_loss: 1.5444\n",
      "Epoch 47/200\n",
      "144/144 [==============================] - 0s 148us/step - loss: 1.0700 - val_loss: 1.4768\n",
      "Epoch 48/200\n",
      "144/144 [==============================] - 0s 146us/step - loss: 1.0706 - val_loss: 1.4865\n",
      "Epoch 49/200\n",
      "144/144 [==============================] - 0s 140us/step - loss: 1.0350 - val_loss: 1.3919\n",
      "Epoch 50/200\n",
      "144/144 [==============================] - 0s 136us/step - loss: 1.0898 - val_loss: 1.7410\n",
      "Epoch 51/200\n",
      "144/144 [==============================] - 0s 146us/step - loss: 1.1027 - val_loss: 1.0838\n",
      "Epoch 52/200\n",
      "144/144 [==============================] - 0s 131us/step - loss: 0.9952 - val_loss: 1.1872\n",
      "Epoch 53/200\n",
      "144/144 [==============================] - 0s 155us/step - loss: 0.9783 - val_loss: 1.0560\n",
      "Epoch 54/200\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.9464 - val_loss: 1.2388\n",
      "Epoch 55/200\n",
      "144/144 [==============================] - 0s 128us/step - loss: 0.9636 - val_loss: 1.0826\n",
      "Epoch 56/200\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.9428 - val_loss: 1.1172\n",
      "Epoch 57/200\n",
      "144/144 [==============================] - 0s 159us/step - loss: 0.9402 - val_loss: 1.7764\n",
      "Epoch 58/200\n",
      "144/144 [==============================] - 0s 154us/step - loss: 1.0334 - val_loss: 1.2072\n",
      "Epoch 59/200\n",
      "144/144 [==============================] - 0s 133us/step - loss: 0.9344 - val_loss: 1.0745\n",
      "Epoch 60/200\n",
      "144/144 [==============================] - 0s 228us/step - loss: 0.9251 - val_loss: 1.1507\n",
      "Epoch 61/200\n",
      "144/144 [==============================] - 0s 104us/step - loss: 0.9053 - val_loss: 1.0398\n",
      "Epoch 62/200\n",
      "144/144 [==============================] - 0s 150us/step - loss: 0.9056 - val_loss: 0.9980\n",
      "Epoch 63/200\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.8911 - val_loss: 0.9987\n",
      "Epoch 64/200\n",
      "144/144 [==============================] - 0s 113us/step - loss: 0.8920 - val_loss: 1.2106\n",
      "Epoch 65/200\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.9265 - val_loss: 1.0583\n",
      "Epoch 66/200\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8604 - val_loss: 1.1565\n",
      "Epoch 67/200\n",
      "144/144 [==============================] - 0s 100us/step - loss: 0.9303 - val_loss: 1.0215\n",
      "Epoch 68/200\n",
      "144/144 [==============================] - 0s 121us/step - loss: 0.8610 - val_loss: 0.9897\n",
      "Epoch 69/200\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8933 - val_loss: 0.9712\n",
      "Epoch 70/200\n",
      "144/144 [==============================] - 0s 141us/step - loss: 0.8625 - val_loss: 1.0275\n",
      "Epoch 71/200\n",
      "144/144 [==============================] - 0s 112us/step - loss: 0.8691 - val_loss: 0.9730\n",
      "Epoch 72/200\n",
      "144/144 [==============================] - 0s 99us/step - loss: 0.8527 - val_loss: 0.9617\n",
      "Epoch 73/200\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8722 - val_loss: 0.9607\n",
      "Epoch 74/200\n",
      "144/144 [==============================] - 0s 220us/step - loss: 0.8400 - val_loss: 1.0157\n",
      "Epoch 75/200\n",
      "144/144 [==============================] - 0s 111us/step - loss: 0.9022 - val_loss: 1.0532\n",
      "Epoch 76/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 0.8381 - val_loss: 1.1233\n",
      "Epoch 77/200\n",
      "144/144 [==============================] - 0s 153us/step - loss: 0.8983 - val_loss: 1.1382\n",
      "Epoch 78/200\n",
      "144/144 [==============================] - 0s 153us/step - loss: 0.8690 - val_loss: 1.0313\n",
      "Epoch 79/200\n",
      "144/144 [==============================] - 0s 134us/step - loss: 0.8556 - val_loss: 0.9691\n",
      "Epoch 80/200\n",
      "144/144 [==============================] - 0s 200us/step - loss: 0.8383 - val_loss: 1.2890\n",
      "Epoch 81/200\n",
      "144/144 [==============================] - 0s 183us/step - loss: 0.8775 - val_loss: 0.9404\n",
      "Epoch 82/200\n",
      "144/144 [==============================] - 0s 230us/step - loss: 0.8403 - val_loss: 0.9763\n",
      "Epoch 83/200\n",
      "144/144 [==============================] - 0s 222us/step - loss: 0.8931 - val_loss: 0.9436\n",
      "Epoch 84/200\n",
      "144/144 [==============================] - 0s 135us/step - loss: 0.8102 - val_loss: 1.6473\n",
      "Epoch 85/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.9623 - val_loss: 0.9813\n",
      "Epoch 86/200\n",
      "144/144 [==============================] - 0s 90us/step - loss: 0.8462 - val_loss: 0.9385\n",
      "Epoch 87/200\n",
      "144/144 [==============================] - 0s 145us/step - loss: 0.8562 - val_loss: 0.9356\n",
      "Epoch 88/200\n",
      "144/144 [==============================] - 0s 189us/step - loss: 0.8401 - val_loss: 1.1897\n",
      "Epoch 89/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 0.8692 - val_loss: 1.0128\n",
      "Epoch 90/200\n",
      "144/144 [==============================] - 0s 126us/step - loss: 0.8205 - val_loss: 1.0342\n",
      "Epoch 91/200\n",
      "144/144 [==============================] - 0s 188us/step - loss: 0.8657 - val_loss: 0.9924\n",
      "Epoch 92/200\n",
      "144/144 [==============================] - 0s 118us/step - loss: 0.8348 - val_loss: 1.2675\n",
      "Epoch 93/200\n",
      "144/144 [==============================] - 0s 92us/step - loss: 0.8940 - val_loss: 0.9846\n",
      "Epoch 94/200\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.8276 - val_loss: 1.0458\n",
      "Epoch 95/200\n",
      "144/144 [==============================] - 0s 139us/step - loss: 0.8680 - val_loss: 1.0655\n",
      "Epoch 96/200\n",
      "144/144 [==============================] - 0s 158us/step - loss: 0.8698 - val_loss: 0.9301\n",
      "Epoch 97/200\n",
      "144/144 [==============================] - 0s 143us/step - loss: 0.8284 - val_loss: 0.9263\n",
      "Epoch 98/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.8191 - val_loss: 0.9591\n",
      "Epoch 99/200\n",
      "144/144 [==============================] - 0s 140us/step - loss: 0.8389 - val_loss: 0.9312\n",
      "Epoch 100/200\n",
      "144/144 [==============================] - 0s 129us/step - loss: 0.8033 - val_loss: 0.9495\n",
      "Epoch 101/200\n",
      "144/144 [==============================] - 0s 146us/step - loss: 0.8017 - val_loss: 0.9813\n",
      "Epoch 102/200\n",
      "144/144 [==============================] - 0s 127us/step - loss: 0.8353 - val_loss: 1.0833\n",
      "Epoch 103/200\n",
      "144/144 [==============================] - 0s 111us/step - loss: 0.8545 - val_loss: 1.0792\n",
      "Epoch 104/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 0.8433 - val_loss: 0.9391\n",
      "Epoch 105/200\n",
      "144/144 [==============================] - 0s 135us/step - loss: 0.8113 - val_loss: 0.9465\n",
      "Epoch 106/200\n",
      "144/144 [==============================] - 0s 118us/step - loss: 0.7965 - val_loss: 0.9467\n",
      "Epoch 107/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.8211 - val_loss: 0.9105\n",
      "Epoch 108/200\n",
      "144/144 [==============================] - 0s 125us/step - loss: 0.8073 - val_loss: 0.9159\n",
      "Epoch 109/200\n",
      "144/144 [==============================] - 0s 139us/step - loss: 0.8354 - val_loss: 1.0366\n",
      "Epoch 110/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 0.8731 - val_loss: 1.0899\n",
      "Epoch 111/200\n",
      "144/144 [==============================] - 0s 122us/step - loss: 0.8239 - val_loss: 0.9280\n",
      "Epoch 112/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 0.8110 - val_loss: 0.9579\n",
      "Epoch 113/200\n",
      "144/144 [==============================] - 0s 106us/step - loss: 0.8201 - val_loss: 1.3021\n",
      "Epoch 114/200\n",
      "144/144 [==============================] - 0s 128us/step - loss: 0.8958 - val_loss: 1.1162\n",
      "Epoch 115/200\n",
      "144/144 [==============================] - 0s 112us/step - loss: 0.7783 - val_loss: 1.0274\n",
      "Epoch 116/200\n",
      "144/144 [==============================] - 0s 118us/step - loss: 0.8198 - val_loss: 0.9913\n",
      "Epoch 117/200\n",
      "144/144 [==============================] - 0s 129us/step - loss: 0.8039 - val_loss: 0.9361\n",
      "Epoch 118/200\n",
      "144/144 [==============================] - 0s 113us/step - loss: 0.7898 - val_loss: 1.0331\n",
      "Epoch 119/200\n",
      "144/144 [==============================] - 0s 112us/step - loss: 0.8439 - val_loss: 1.4612\n",
      "Epoch 120/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.8929 - val_loss: 1.1608\n",
      "Epoch 121/200\n",
      "144/144 [==============================] - 0s 104us/step - loss: 0.8149 - val_loss: 0.9106\n",
      "Epoch 122/200\n",
      "144/144 [==============================] - 0s 93us/step - loss: 0.8037 - val_loss: 0.9032\n",
      "Epoch 123/200\n",
      "144/144 [==============================] - 0s 85us/step - loss: 0.8492 - val_loss: 1.0957\n",
      "Epoch 124/200\n",
      "144/144 [==============================] - 0s 118us/step - loss: 0.8765 - val_loss: 1.0453\n",
      "Epoch 125/200\n",
      "144/144 [==============================] - 0s 116us/step - loss: 0.9058 - val_loss: 0.9088\n",
      "Epoch 126/200\n",
      "144/144 [==============================] - 0s 111us/step - loss: 0.8243 - val_loss: 0.9108\n",
      "Epoch 127/200\n",
      "144/144 [==============================] - 0s 137us/step - loss: 0.8383 - val_loss: 1.3047\n",
      "Epoch 128/200\n",
      "144/144 [==============================] - 0s 116us/step - loss: 0.8683 - val_loss: 1.0649\n",
      "Epoch 129/200\n",
      "144/144 [==============================] - 0s 118us/step - loss: 0.8438 - val_loss: 0.9979\n",
      "Epoch 130/200\n",
      "144/144 [==============================] - 0s 137us/step - loss: 0.7992 - val_loss: 1.0205\n",
      "Epoch 131/200\n",
      "144/144 [==============================] - 0s 121us/step - loss: 0.8906 - val_loss: 1.0582\n",
      "Epoch 132/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 0.7976 - val_loss: 0.9331\n",
      "Epoch 133/200\n",
      "144/144 [==============================] - 0s 190us/step - loss: 0.7913 - val_loss: 0.9101\n",
      "Epoch 134/200\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8247 - val_loss: 0.9032\n",
      "Epoch 135/200\n",
      "144/144 [==============================] - 0s 137us/step - loss: 0.8280 - val_loss: 0.9153\n",
      "Epoch 136/200\n",
      "144/144 [==============================] - 0s 142us/step - loss: 0.8365 - val_loss: 0.9098\n",
      "Epoch 137/200\n",
      "144/144 [==============================] - 0s 94us/step - loss: 0.8090 - val_loss: 1.6192\n",
      "Epoch 138/200\n",
      "144/144 [==============================] - 0s 76us/step - loss: 0.8817 - val_loss: 0.8982\n",
      "Epoch 139/200\n",
      "144/144 [==============================] - 0s 115us/step - loss: 0.7976 - val_loss: 0.9068\n",
      "Epoch 140/200\n",
      "144/144 [==============================] - 0s 116us/step - loss: 0.8369 - val_loss: 1.0275\n",
      "Epoch 141/200\n",
      "144/144 [==============================] - 0s 118us/step - loss: 0.8119 - val_loss: 0.9129\n",
      "Epoch 142/200\n",
      "144/144 [==============================] - 0s 91us/step - loss: 0.7940 - val_loss: 0.9137\n",
      "Epoch 143/200\n",
      "144/144 [==============================] - 0s 110us/step - loss: 0.8088 - val_loss: 0.9490\n",
      "Epoch 144/200\n",
      "144/144 [==============================] - 0s 119us/step - loss: 0.8159 - val_loss: 1.1023\n",
      "Epoch 145/200\n",
      "144/144 [==============================] - 0s 92us/step - loss: 0.8062 - val_loss: 1.0027\n",
      "Epoch 146/200\n",
      "144/144 [==============================] - 0s 94us/step - loss: 0.8314 - val_loss: 0.9597\n",
      "Epoch 147/200\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.8315 - val_loss: 0.9165\n",
      "Epoch 148/200\n",
      "144/144 [==============================] - 0s 102us/step - loss: 0.8075 - val_loss: 0.9676\n",
      "Epoch 149/200\n",
      "144/144 [==============================] - 0s 116us/step - loss: 0.8676 - val_loss: 0.9020\n",
      "Epoch 150/200\n",
      "144/144 [==============================] - 0s 127us/step - loss: 0.8072 - val_loss: 1.9934\n",
      "Epoch 151/200\n",
      "144/144 [==============================] - 0s 116us/step - loss: 0.9359 - val_loss: 1.0044\n",
      "Epoch 152/200\n",
      "144/144 [==============================] - 0s 111us/step - loss: 0.9055 - val_loss: 0.8979\n",
      "Epoch 153/200\n",
      "144/144 [==============================] - 0s 148us/step - loss: 0.8063 - val_loss: 0.8965\n",
      "Epoch 154/200\n",
      "144/144 [==============================] - 0s 141us/step - loss: 0.8283 - val_loss: 1.3022\n",
      "Epoch 155/200\n",
      "144/144 [==============================] - 0s 136us/step - loss: 0.8871 - val_loss: 0.9653\n",
      "Epoch 156/200\n",
      "144/144 [==============================] - 0s 133us/step - loss: 0.8548 - val_loss: 0.9963\n",
      "Epoch 157/200\n",
      "144/144 [==============================] - 0s 125us/step - loss: 0.7990 - val_loss: 0.8972\n",
      "Epoch 158/200\n",
      "144/144 [==============================] - 0s 85us/step - loss: 0.8073 - val_loss: 1.0070\n",
      "Epoch 159/200\n",
      "144/144 [==============================] - 0s 100us/step - loss: 0.8440 - val_loss: 0.9562\n",
      "Epoch 160/200\n",
      "144/144 [==============================] - 0s 90us/step - loss: 0.8178 - val_loss: 1.4572\n",
      "Epoch 161/200\n",
      "144/144 [==============================] - 0s 99us/step - loss: 0.8908 - val_loss: 1.4461\n",
      "Epoch 162/200\n",
      "144/144 [==============================] - 0s 95us/step - loss: 0.8994 - val_loss: 0.8999\n",
      "Epoch 163/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.8052 - val_loss: 0.8975\n",
      "Epoch 164/200\n",
      "144/144 [==============================] - 0s 79us/step - loss: 0.8138 - val_loss: 1.1974\n",
      "Epoch 165/200\n",
      "144/144 [==============================] - 0s 107us/step - loss: 0.8216 - val_loss: 0.8988\n",
      "Epoch 166/200\n",
      "144/144 [==============================] - 0s 152us/step - loss: 0.8054 - val_loss: 0.9878\n",
      "Epoch 167/200\n",
      "144/144 [==============================] - 0s 85us/step - loss: 0.8229 - val_loss: 1.0415\n",
      "Epoch 168/200\n",
      "144/144 [==============================] - 0s 97us/step - loss: 0.8465 - val_loss: 0.9250\n",
      "Epoch 169/200\n",
      "144/144 [==============================] - 0s 116us/step - loss: 0.7958 - val_loss: 0.9084\n",
      "Epoch 170/200\n",
      "144/144 [==============================] - 0s 128us/step - loss: 0.8051 - val_loss: 0.8961\n",
      "Epoch 171/200\n",
      "144/144 [==============================] - 0s 140us/step - loss: 0.8197 - val_loss: 0.9043\n",
      "Epoch 172/200\n",
      "144/144 [==============================] - 0s 87us/step - loss: 0.8117 - val_loss: 0.8956\n",
      "Epoch 173/200\n",
      "144/144 [==============================] - 0s 186us/step - loss: 0.8202 - val_loss: 1.0737\n",
      "Epoch 174/200\n",
      "144/144 [==============================] - 0s 126us/step - loss: 0.8461 - val_loss: 1.0415\n",
      "Epoch 175/200\n",
      "144/144 [==============================] - 0s 132us/step - loss: 0.8181 - val_loss: 0.8996\n",
      "Epoch 176/200\n",
      "144/144 [==============================] - 0s 126us/step - loss: 0.8333 - val_loss: 0.9495\n",
      "Epoch 177/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.8241 - val_loss: 0.9752\n",
      "Epoch 178/200\n",
      "144/144 [==============================] - 0s 86us/step - loss: 0.8699 - val_loss: 2.0136\n",
      "Epoch 179/200\n",
      "144/144 [==============================] - 0s 100us/step - loss: 0.9081 - val_loss: 0.9213\n",
      "Epoch 180/200\n",
      "144/144 [==============================] - 0s 101us/step - loss: 0.8142 - val_loss: 0.9935\n",
      "Epoch 181/200\n",
      "144/144 [==============================] - 0s 104us/step - loss: 0.8458 - val_loss: 0.9075\n",
      "Epoch 182/200\n",
      "144/144 [==============================] - 0s 103us/step - loss: 0.8105 - val_loss: 1.2620\n",
      "Epoch 183/200\n",
      "144/144 [==============================] - 0s 112us/step - loss: 0.8745 - val_loss: 0.9107\n",
      "Epoch 184/200\n",
      "144/144 [==============================] - 0s 100us/step - loss: 0.8355 - val_loss: 0.9086\n",
      "Epoch 185/200\n",
      "144/144 [==============================] - 0s 85us/step - loss: 0.8025 - val_loss: 0.9382\n",
      "Epoch 186/200\n",
      "144/144 [==============================] - 0s 104us/step - loss: 0.8166 - val_loss: 4.3747\n",
      "Epoch 187/200\n",
      "144/144 [==============================] - 0s 104us/step - loss: 1.3940 - val_loss: 1.1282\n",
      "Epoch 188/200\n",
      "144/144 [==============================] - 0s 99us/step - loss: 0.8853 - val_loss: 0.9122\n",
      "Epoch 189/200\n",
      "144/144 [==============================] - 0s 92us/step - loss: 0.7968 - val_loss: 0.9060\n",
      "Epoch 190/200\n",
      "144/144 [==============================] - 0s 79us/step - loss: 0.8071 - val_loss: 0.9124\n",
      "Epoch 191/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.7993 - val_loss: 1.1460\n",
      "Epoch 192/200\n",
      "144/144 [==============================] - 0s 118us/step - loss: 0.8744 - val_loss: 0.9134\n",
      "Epoch 193/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.8204 - val_loss: 0.9428\n",
      "Epoch 194/200\n",
      "144/144 [==============================] - 0s 102us/step - loss: 0.8253 - val_loss: 1.1822\n",
      "Epoch 195/200\n",
      "144/144 [==============================] - 0s 90us/step - loss: 0.8830 - val_loss: 0.9294\n",
      "Epoch 196/200\n",
      "144/144 [==============================] - 0s 84us/step - loss: 0.8138 - val_loss: 1.0611\n",
      "Epoch 197/200\n",
      "144/144 [==============================] - 0s 106us/step - loss: 0.8048 - val_loss: 0.9110\n",
      "Epoch 198/200\n",
      "144/144 [==============================] - 0s 114us/step - loss: 0.8167 - val_loss: 1.0837\n",
      "Epoch 199/200\n",
      "144/144 [==============================] - 0s 83us/step - loss: 0.7941 - val_loss: 0.8993\n",
      "Epoch 200/200\n",
      "144/144 [==============================] - 0s 92us/step - loss: 0.8045 - val_loss: 0.9045\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X[:160],Y[:160],validation_split=0.1,epochs=200,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 150us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9775403022766114"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X[160:],Y[160:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b=model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0],-1)/255\n",
    "X_test=X_test.reshape(X_test.shape[0],-1)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np_utils.to_categorical(Y_train,10)\n",
    "Y_test=np_utils.to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(784,))\n",
    "x=Dense(32,activation='relu')(inp)\n",
    "x=Dense(10,activation='softmax')(x)\n",
    "model=Model(inp,x)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 24s 405us/step - loss: 0.2609 - acc: 0.9226\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 0.1425 - acc: 0.9574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x225c16b2128>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=2,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 111us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12513850409910082, 0.9623000025749207]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(-1,28,28,1)/255\n",
    "X_test=X_test.reshape(-1,28,28,1)/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np_utils.to_categorical(Y_train,10)\n",
    "Y_test=np_utils.to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(28,28,1))\n",
    "x=Conv2D(kernel_size=(5,5),filters=32,strides=1,padding='same')(inp)\n",
    "x=MaxPool2D(pool_size=(2,2),strides=2,padding='same')(x)\n",
    "x=Conv2D(kernel_size=(5,5),filters=64,strides=1,padding='same')(x)\n",
    "x=MaxPool2D(pool_size=(2,2),strides=2,padding='same')(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(1024,activation='relu')(x)\n",
    "x=Dense(10,activation='softmax')(x)\n",
    "model=Model(inp,x)\n",
    "adam=Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,274,634\n",
      "Trainable params: 3,274,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 225s 4ms/step - loss: 0.2036 - accuracy: 0.9437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x225c1024908>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10s 992us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06203326836451888, 0.9800000190734863]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
